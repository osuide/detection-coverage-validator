"""
T1499.004 - Endpoint Denial of Service: Application or System Exploitation

Adversaries exploit software vulnerabilities to crash applications or systems,
denying availability to users. Systems may auto-restart critical services,
enabling persistent DoS conditions through re-exploitation.
Used by Industroyer malware against industrial control systems.
"""

from .template_loader import (
    RemediationTemplate,
    ThreatContext,
    DetectionStrategy,
    DetectionImplementation,
    DetectionType,
    EffortLevel,
    FalsePositiveRate,
    CloudProvider,
)

TEMPLATE = RemediationTemplate(
    technique_id="T1499.004",
    technique_name="Endpoint Denial of Service: Application or System Exploitation",
    tactic_ids=["TA0040"],
    mitre_url="https://attack.mitre.org/techniques/T1499/004/",
    threat_context=ThreatContext(
        description=(
            "Adversaries leverage software vulnerabilities to crash applications or systems, "
            "denying availability to users. They may exploit known CVEs or zero-day vulnerabilities "
            "to cause service crashes, restarts, or repeated failures. In cloud environments, "
            "this targets web applications, APIs, containers, and serverless functions. "
            "Systems may auto-restart critical services, but re-exploitation creates persistent "
            "DoS conditions. Secondary effects can include data destruction, firmware corruption, "
            "or service termination in virtual machines and container workloads."
        ),
        attacker_goal="Exploit vulnerabilities to crash services and deny availability",
        why_technique=[
            "Exploits bypass traditional rate-limiting defences",
            "Auto-restart mechanisms enable persistent DoS",
            "Can trigger container/VM termination cycles",
            "May cause data loss or corruption",
            "Difficult to distinguish from legitimate bugs",
        ],
        known_threat_actors=[],
        recent_campaigns=[],  # Populated dynamically from MITRE sync data
        prevalence="moderate",
        trend="stable",
        severity_score=8,
        severity_reasoning=(
            "High impact on availability with potential for persistent service disruption. "
            "Exploitation can cause crashes, data loss, and require manual intervention. "
            "In cloud environments, can trigger excessive restart cycles and resource exhaustion. "
            "Particularly severe for critical infrastructure and industrial control systems."
        ),
        business_impact=[
            "Service unavailability and outages",
            "Data loss or corruption from crashes",
            "Excessive cloud costs from restart cycles",
            "Manual intervention required for recovery",
            "Potential safety impacts in critical systems",
        ],
        typical_attack_phase="impact",
        often_precedes=[],
        often_follows=["T1190", "T1595.002"],
    ),
    detection_strategies=[
        DetectionStrategy(
            strategy_id="t1499.004-aws-application-crashes",
            name="AWS Application Crash Detection",
            description="Detect repeated application crashes or errors indicating exploitation attempts.",
            detection_type=DetectionType.CLOUDWATCH_QUERY,
            aws_service="cloudwatch",
            cloud_provider=CloudProvider.AWS,
            implementation=DetectionImplementation(
                query="""fields @timestamp, @message, @logStream
| filter @message like /SIGSEGV|segmentation fault|fatal error|panic|unhandled exception/
| stats count(*) as crash_count by @logStream, bin(10m)
| filter crash_count > 3
| sort crash_count desc""",
                cloudformation_template="""AWSTemplateFormatVersion: '2010-09-09'
Description: Detect application crashes from exploitation

Parameters:
  ApplicationLogGroup:
    Type: String
    Description: CloudWatch Log Group for application logs
  AlertEmail:
    Type: String
    Description: Email address for alerts

Resources:
  AlertTopic:
    Type: AWS::SNS::Topic
    Properties:
      KmsMasterKeyId: alias/aws/sns
      Subscription:
        - Protocol: email
          Endpoint: !Ref AlertEmail

  # Metric filter for application crashes
  CrashMetricFilter:
    Type: AWS::Logs::MetricFilter
    Properties:
      LogGroupName: !Ref ApplicationLogGroup
      FilterPattern: '{ $.message = "*SIGSEGV*" || $.message = "*segmentation fault*" || $.message = "*fatal error*" || $.message = "*panic*" }'
      MetricTransformations:
        - MetricName: ApplicationCrashes
          MetricNamespace: Security
          MetricValue: "1"

  # Alarm for repeated crashes
  CrashAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: Application-Exploitation-Crashes
      MetricName: ApplicationCrashes
      Namespace: Security
      Statistic: Sum
      Period: 600
      EvaluationPeriods: 1
      Threshold: 5
      ComparisonOperator: GreaterThanThreshold
      TreatMissingData: notBreaching
      TreatMissingData: notBreaching

      AlarmActions:
        - !Ref AlertTopic
      AlarmDescription: Detects repeated application crashes indicating exploitation""",
                terraform_template="""# Detect application crashes from exploitation

variable "application_log_group" { type = string }
variable "alert_email" { type = string }

resource "aws_sns_topic" "alerts" {
  name = "app-exploitation-alerts"
  kms_master_key_id = "alias/aws/sns"
}

resource "aws_sns_topic_subscription" "email" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

# Metric filter for crashes
resource "aws_cloudwatch_log_metric_filter" "crashes" {
  name           = "application-crashes"
  log_group_name = var.application_log_group
  pattern        = "{ $.message = \"*SIGSEGV*\" || $.message = \"*segmentation fault*\" || $.message = \"*fatal error*\" || $.message = \"*panic*\" }"

  metric_transformation {
    name      = "ApplicationCrashes"
    namespace = "Security"
    value     = "1"
  }
}

# Alarm for repeated crashes
resource "aws_cloudwatch_metric_alarm" "exploitation_crashes" {
  alarm_name          = "Application-Exploitation-Crashes"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "ApplicationCrashes"
  namespace           = "Security"
  period              = 600
  statistic           = "Sum"
  threshold           = 5
  treat_missing_data  = "notBreaching"

  alarm_actions [aws_sns_topic.alerts.arn]
  alarm_description   = "Detects repeated application crashes indicating exploitation"
}""",
                alert_severity="high",
                alert_title="Application Exploitation Detected",
                alert_description_template="Repeated application crashes detected - possible exploitation attempt.",
                investigation_steps=[
                    "Review application logs for crash patterns and stack traces",
                    "Check for CVE matches in error messages",
                    "Analyse source IPs of recent requests before crashes",
                    "Review application version for known vulnerabilities",
                    "Examine core dumps if available",
                ],
                containment_actions=[
                    "Apply security patches immediately",
                    "Block attacking IPs via WAF or security groups",
                    "Enable AWS WAF managed rules for known exploits",
                    "Implement input validation and sanitisation",
                    "Consider rolling back to stable version",
                ],
            ),
            estimated_false_positive_rate=FalsePositiveRate.MEDIUM,
            false_positive_tuning="Exclude known application bugs, adjust threshold based on normal crash rate",
            detection_coverage="75% - detects crash-based exploitation",
            evasion_considerations="Exploitation causing silent failures without crashes may evade",
            implementation_effort=EffortLevel.MEDIUM,
            implementation_time="1 hour",
            estimated_monthly_cost="$5-10",
            prerequisites=["Application logging to CloudWatch"],
        ),
        DetectionStrategy(
            strategy_id="t1499.004-aws-ecs-task-failures",
            name="AWS ECS Task Restart Detection",
            description="Detect repeated ECS task or container failures indicating exploitation.",
            detection_type=DetectionType.CLOUDWATCH_QUERY,
            aws_service="cloudwatch",
            cloud_provider=CloudProvider.AWS,
            implementation=DetectionImplementation(
                query="""fields @timestamp, eventName, detail.stoppedReason, detail.containers.reason
| filter eventName = "ECS Task State Change"
| filter detail.lastStatus = "STOPPED" and detail.stoppedReason like /error|exploit|crash/
| stats count(*) as stop_count by detail.taskDefinitionArn, bin(10m)
| filter stop_count > 3
| sort stop_count desc""",
                terraform_template="""# Detect ECS container exploitation via repeated failures

variable "alert_email" { type = string }

resource "aws_sns_topic" "alerts" {
  name = "ecs-exploitation-alerts"
  kms_master_key_id = "alias/aws/sns"
}

resource "aws_sns_topic_subscription" "email" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

# EventBridge rule for task state changes
resource "aws_cloudwatch_event_rule" "ecs_task_stopped" {
  name        = "ecs-task-exploitation-detection"
  description = "Detect repeated ECS task failures"

  event_pattern = jsonencode({
    source      = ["aws.ecs"]
    detail-type = ["ECS Task State Change"]
    detail = {
      lastStatus = ["STOPPED"]
    }
  })
}

resource "aws_cloudwatch_event_target" "lambda" {
  rule      = aws_cloudwatch_event_rule.ecs_task_stopped.name
  target_id = "SendToSNS"
  arn       = aws_sns_topic.alerts.arn
}

# Allow EventBridge to publish to SNS
resource "aws_sns_topic_policy" "allow_eventbridge" {
  arn = aws_sns_topic.alerts.arn
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect    = "Allow"
      Principal = { Service = "events.amazonaws.com" }
      Action    = "sns:Publish"
      Resource  = aws_sns_topic.alerts.arn
    }]
  })
}

# Metric for task failures
resource "aws_cloudwatch_log_metric_filter" "ecs_failures" {
  name           = "ecs-task-failures"
  log_group_name = "/aws/ecs/containerinsights"  # Adjust to your log group
  pattern        = "[time, stream, message = *SIGSEGV* || message = *crash* || message = *exploit*]"

  metric_transformation {
    name      = "ECSTaskExploitation"
    namespace = "Security"
    value     = "1"
  }
}

# Alarm for repeated failures
resource "aws_cloudwatch_metric_alarm" "ecs_exploitation" {
  alarm_name          = "ECS-Task-Exploitation"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "ECSTaskExploitation"
  namespace           = "Security"
  period              = 600
  statistic           = "Sum"
  threshold           = 3
  treat_missing_data  = "notBreaching"

  alarm_actions [aws_sns_topic.alerts.arn]
  alarm_description   = "Detects repeated ECS task failures from exploitation"
}""",
                alert_severity="high",
                alert_title="ECS Container Exploitation",
                alert_description_template="Repeated ECS task failures detected - possible exploitation attempt.",
                investigation_steps=[
                    "Review ECS task logs for failure reasons",
                    "Check container images for vulnerabilities",
                    "Review stopped task details in ECS console",
                    "Analyse Application Load Balancer access logs",
                    "Check for CVE matches in container software",
                ],
                containment_actions=[
                    "Update container images with security patches",
                    "Block attacking IPs via security groups or WAF",
                    "Implement container security scanning",
                    "Review and harden container configurations",
                    "Consider using AWS Fargate for isolation",
                ],
            ),
            estimated_false_positive_rate=FalsePositiveRate.MEDIUM,
            false_positive_tuning="Exclude deployment-related failures, adjust threshold for legitimate restart patterns",
            detection_coverage="80% - detects container-specific exploitation",
            evasion_considerations="Graceful container exits may not trigger detection",
            implementation_effort=EffortLevel.MEDIUM,
            implementation_time="1-2 hours",
            estimated_monthly_cost="$5-15",
            prerequisites=["ECS Container Insights enabled, CloudWatch logging"],
        ),
        DetectionStrategy(
            strategy_id="t1499.004-aws-lambda-errors",
            name="AWS Lambda Exploitation Detection",
            description="Detect Lambda function errors caused by malformed input or exploitation attempts.",
            detection_type=DetectionType.CLOUDWATCH_QUERY,
            aws_service="cloudwatch",
            cloud_provider=CloudProvider.AWS,
            implementation=DetectionImplementation(
                query="""fields @timestamp, @message, @requestId
| filter @type = "platform.fault" or @message like /Error|Exception|Crash/
| stats count(*) as error_count by bin(5m)
| filter error_count > 10
| sort error_count desc""",
                terraform_template="""# Detect Lambda function exploitation

variable "alert_email" { type = string }

resource "aws_sns_topic" "alerts" {
  name = "lambda-exploitation-alerts"
  kms_master_key_id = "alias/aws/sns"
}

resource "aws_sns_topic_subscription" "email" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

# Alarm for Lambda errors spike
resource "aws_cloudwatch_metric_alarm" "lambda_exploitation" {
  alarm_name          = "Lambda-Exploitation-Errors"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "Errors"
  namespace           = "AWS/Lambda"
  period              = 300
  statistic           = "Sum"
  threshold           = 50
  treat_missing_data  = "notBreaching"

  alarm_actions [aws_sns_topic.alerts.arn]
  alarm_description   = "Detects spike in Lambda errors from exploitation"
  treat_missing_data  = "notBreaching"
}

# Metric for platform faults (crashes)
resource "aws_cloudwatch_metric_alarm" "lambda_crashes" {
  alarm_name          = "Lambda-Crash-Detection"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "DeadLetterErrors"
  namespace           = "AWS/Lambda"
  period              = 300
  statistic           = "Sum"
  threshold           = 10
  treat_missing_data  = "notBreaching"

  alarm_actions [aws_sns_topic.alerts.arn]
  alarm_description   = "Detects Lambda function crashes"
  treat_missing_data  = "notBreaching"
}""",
                alert_severity="high",
                alert_title="Lambda Function Exploitation",
                alert_description_template="Spike in Lambda errors - possible exploitation via malformed input.",
                investigation_steps=[
                    "Review Lambda function logs for error patterns",
                    "Analyse X-Ray traces for failure points",
                    "Check recent invocation payloads",
                    "Review function dependencies for vulnerabilities",
                    "Examine dead-letter queue messages",
                ],
                containment_actions=[
                    "Update function runtime and dependencies",
                    "Implement input validation and sanitisation",
                    "Enable AWS WAF on API Gateway trigger",
                    "Configure reserved concurrency limits",
                    "Review and restrict function permissions",
                ],
            ),
            estimated_false_positive_rate=FalsePositiveRate.MEDIUM,
            false_positive_tuning="Adjust error thresholds based on normal function behaviour, exclude deployment periods",
            detection_coverage="70% - detects Lambda-specific exploitation",
            evasion_considerations="Exploitation causing timeouts rather than errors may evade",
            implementation_effort=EffortLevel.LOW,
            implementation_time="30 minutes",
            estimated_monthly_cost="$2-5",
            prerequisites=["CloudWatch Logs enabled for Lambda"],
        ),
        DetectionStrategy(
            strategy_id="t1499.004-gcp-crash-detection",
            name="GCP Application Crash Detection",
            description="Detect repeated application crashes on GCE and GKE indicating exploitation.",
            detection_type=DetectionType.CLOUD_LOGGING_QUERY,
            aws_service="n/a",
            gcp_service="cloud_logging",
            cloud_provider=CloudProvider.GCP,
            implementation=DetectionImplementation(
                gcp_logging_query="""(resource.type="gce_instance" OR resource.type="k8s_container" OR resource.type="k8s_pod")
AND (
  jsonPayload.message=~"SIGSEGV|segmentation fault|fatal error|panic"
  OR severity="ERROR"
  OR jsonPayload.signal="SIGSEGV"
)""",
                gcp_terraform_template="""# GCP: Detect application exploitation via crashes

variable "project_id" { type = string }
variable "alert_email" { type = string }

resource "google_monitoring_notification_channel" "email" {
  display_name = "Security Alerts"
  type         = "email"
  labels       = { email_address = var.alert_email }
}

# Logging metric for application crashes
resource "google_logging_metric" "app_crashes" {
  name   = "application-exploitation-crashes"
  filter = <<-EOT
    (resource.type="gce_instance" OR resource.type="k8s_container" OR resource.type="k8s_pod")
    AND (
      jsonPayload.message=~"SIGSEGV|segmentation fault|fatal error|panic"
      OR jsonPayload.signal="SIGSEGV"
    )
  EOT
  metric_descriptor {
    metric_kind = "DELTA"
    value_type  = "INT64"
    labels {
      key         = "resource_name"
      value_type  = "STRING"
      description = "Resource experiencing crashes"
    }
  }
  label_extractors = {
    "resource_name" = "EXTRACT(resource.labels.instance_id)"
  }
}

# Alert for repeated crashes
resource "google_monitoring_alert_policy" "crash_alert" {
  display_name = "Application Exploitation Crashes"
  combiner     = "OR"
  conditions {
    display_name = "Repeated crashes detected"
    condition_threshold {
      filter          = "metric.type=\"logging.googleapis.com/user/${google_logging_metric.app_crashes.name}\""
      duration        = "300s"
      comparison      = "COMPARISON_GT"
      threshold_value = 5
      aggregations {
        alignment_period   = "60s"
        per_series_aligner = "ALIGN_RATE"
      }
    }
  }
  notification_channels = [google_monitoring_notification_channel.email.id]
  documentation {
    content = "Repeated application crashes detected - possible exploitation attempt"
  }
}""",
                alert_severity="high",
                alert_title="GCP: Application Exploitation Detected",
                alert_description_template="Repeated application crashes on GCP resources - possible exploitation.",
                investigation_steps=[
                    "Review Cloud Logging for crash patterns and stack traces",
                    "Check GKE pod status and restart counts",
                    "Analyse Cloud Load Balancing logs for suspicious requests",
                    "Review container images for known vulnerabilities",
                    "Examine Cloud Trace for error patterns",
                ],
                containment_actions=[
                    "Apply security patches to affected instances",
                    "Block attacking IPs via Cloud Armour or firewall rules",
                    "Update container images with patched versions",
                    "Implement input validation in applications",
                    "Enable Cloud Armour with OWASP rules",
                ],
            ),
            estimated_false_positive_rate=FalsePositiveRate.MEDIUM,
            false_positive_tuning="Filter out known application bugs, adjust crash threshold for workload patterns",
            detection_coverage="75% - detects crash-based exploitation",
            evasion_considerations="Silent failures or graceful degradation may not trigger alerts",
            implementation_effort=EffortLevel.MEDIUM,
            implementation_time="1 hour",
            estimated_monthly_cost="$10-15",
            prerequisites=["Cloud Logging enabled"],
        ),
        DetectionStrategy(
            strategy_id="t1499.004-gcp-pod-restarts",
            name="GCP GKE Pod Restart Detection",
            description="Detect excessive pod restart cycles indicating container exploitation.",
            detection_type=DetectionType.CLOUD_LOGGING_QUERY,
            aws_service="n/a",
            gcp_service="cloud_monitoring",
            cloud_provider=CloudProvider.GCP,
            implementation=DetectionImplementation(
                gcp_logging_query='''resource.type="k8s_pod"
protoPayload.methodName="io.k8s.core.v1.pods.update"
jsonPayload.reason="CrashLoopBackOff" OR jsonPayload.reason="Error"''',
                gcp_terraform_template="""# GCP: Detect GKE pod exploitation via restarts

variable "project_id" { type = string }
variable "alert_email" { type = string }
variable "cluster_name" { type = string }

resource "google_monitoring_notification_channel" "email" {
  display_name = "Security Alerts"
  type         = "email"
  labels       = { email_address = var.alert_email }
}

# Alert for pod restart rate
resource "google_monitoring_alert_policy" "pod_restarts" {
  display_name = "GKE Pod Exploitation Restarts"
  combiner     = "OR"
  conditions {
    display_name = "High pod restart rate"
    condition_threshold {
      filter          = "resource.type=\"k8s_pod\" AND metric.type=\"kubernetes.io/pod/restart_count\""
      duration        = "300s"
      comparison      = "COMPARISON_GT"
      threshold_value = 5
      aggregations {
        alignment_period     = "60s"
        per_series_aligner   = "ALIGN_DELTA"
        cross_series_reducer = "REDUCE_SUM"
        group_by_fields      = ["resource.pod_name", "resource.namespace_name"]
      }
    }
  }
  notification_channels = [google_monitoring_notification_channel.email.id]
  documentation {
    content = "Excessive pod restarts detected - possible container exploitation"
  }
}

# Logging metric for CrashLoopBackOff
resource "google_logging_metric" "crash_loop" {
  name   = "pod-crash-loop"
  filter = <<-EOT
    resource.type="k8s_pod"
    jsonPayload.reason="CrashLoopBackOff"
  EOT
  metric_descriptor {
    metric_kind = "DELTA"
    value_type  = "INT64"
  }
}

# Alert for crash loops
resource "google_monitoring_alert_policy" "crash_loop_alert" {
  display_name = "GKE CrashLoopBackOff Detection"
  combiner     = "OR"
  conditions {
    display_name = "Pods in crash loop"
    condition_threshold {
      filter          = "metric.type=\"logging.googleapis.com/user/${google_logging_metric.crash_loop.name}\""
      duration        = "0s"
      comparison      = "COMPARISON_GT"
      threshold_value = 3
    }
  }
  notification_channels = [google_monitoring_notification_channel.email.id]
}""",
                alert_severity="high",
                alert_title="GCP: GKE Pod Exploitation",
                alert_description_template="Excessive GKE pod restarts - possible container exploitation.",
                investigation_steps=[
                    "Check pod logs with kubectl logs",
                    "Review pod events with kubectl describe pod",
                    "Analyse container exit codes and reasons",
                    "Scan container images for vulnerabilities",
                    "Review ingress/service logs for malicious traffic",
                ],
                containment_actions=[
                    "Update container images with patches",
                    "Implement pod security policies/standards",
                    "Configure resource limits and quotas",
                    "Enable Binary Authorisation for image validation",
                    "Block malicious sources via Network Policies",
                ],
            ),
            estimated_false_positive_rate=FalsePositiveRate.LOW,
            false_positive_tuning="Exclude deployment rollouts, adjust restart threshold for application patterns",
            detection_coverage="85% - detects GKE-specific exploitation",
            evasion_considerations="Exploitation causing pod hangs rather than restarts may evade",
            implementation_effort=EffortLevel.MEDIUM,
            implementation_time="1 hour",
            estimated_monthly_cost="$10-15",
            prerequisites=["GKE cluster with Cloud Monitoring enabled"],
        ),
    ],
    recommended_order=[
        "t1499.004-aws-application-crashes",
        "t1499.004-aws-ecs-task-failures",
        "t1499.004-aws-lambda-errors",
        "t1499.004-gcp-crash-detection",
        "t1499.004-gcp-pod-restarts",
    ],
    total_effort_hours=5.5,
    coverage_improvement="+12% improvement for Impact tactic",
)
